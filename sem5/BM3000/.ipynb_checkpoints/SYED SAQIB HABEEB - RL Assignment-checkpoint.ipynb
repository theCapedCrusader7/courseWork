{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68be25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b274a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in /home/saqib/.local/lib/python3.8/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6303f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAgent:\n",
    "    '''Abstract class for all agents. '''\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def step(self):\n",
    "        '''Take an action '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef6661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractEnvironment:\n",
    "    '''Base class for all enviornments.'''\n",
    "\n",
    "    def reset(self):\n",
    "        '''Reset the state of the environment to original.\n",
    "\n",
    "        Returns:\n",
    "        The new state\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, action):\n",
    "        '''Run the enivornment one step by taking an action.\n",
    "\n",
    "        Arguments:\n",
    "        action - The action to take\n",
    "\n",
    "        Returns:\n",
    "        The amount of reward recieved\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_num_states(self):\n",
    "        '''Get the number of states in the environment'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_num_actions(self):\n",
    "        '''Get the number of actions the agent can perform'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_current_state(self):\n",
    "        '''Get the current state of the environment.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_state_space(self):\n",
    "        '''Get the possible states of the environment'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action_space(self):\n",
    "        '''Get the possible actions in the environment'''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18fe305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMaze(AbstractEnvironment):\n",
    "    #TODO make a 5X5 grid\n",
    "\n",
    "    default_maze = [\"#G...\",\n",
    "                    \"#..#.\",\n",
    "                    \".....\",\n",
    "                    \"..#..\",\n",
    "                    \"##.\",\n",
    "                    \"..A..\"]\n",
    "\n",
    "    action_movements = {\n",
    "        #TODO map the remaining set of actions to Integer domain\n",
    "        'N': np.array((-1, 0)),\n",
    "        'W': np.array((1, 0)),\n",
    "        'S': np.array((0, -1)),\n",
    "        'E': np.array((0, 1))\n",
    "        \n",
    "    }\n",
    "\n",
    "    def _init_(self, maze=None):\n",
    "        if maze is None:\n",
    "            maze = self.default_maze.copy()\n",
    "        self.maze = np.array(list(map(list, maze)))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        #TODO move the agent back to its starting position\n",
    "        self.agent_pos = np.array((4,2))\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # if action not in self.action_movements.keys():\n",
    "        # raise ValueError(f\"Invalid action {action}. Please use N, W, S or E.\")\n",
    "        \n",
    "        if (action!=1 and action!=2 and action!=0 and action!=3):\n",
    "          raise ValueError(f\"Invalid action {action}. Please use N, W, S or E.\")\n",
    "        proxy_pos = self.agent_pos + self.action_movements[action]\n",
    "        if ((proxy_pos)[0]<=4) and ((proxy_pos)[0]>=0) and ((proxy_pos)[1]<=4) and ((proxy_pos)[1]>=0):\n",
    "          #TODO determine the new position of the agent\n",
    "          new_pos = proxy_pos\n",
    "        if proxy_pos[0] > 4 or proxy_pos[1] > 4:\n",
    "          # TODO update new position when the agent reaches the border\n",
    "          new_pos = self.agent_pos         \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # TODO update new position when the agent hits an obstucle next\n",
    "        if self.maze[new_pos[0]][new_pos[1]] == \"#\":\n",
    "          new_pos = self.reset()\n",
    "        \n",
    "        self.agent_pos = new_pos\n",
    "        new_pos_char = self.maze[new_pos[0]][new_pos[1]]\n",
    "        #TODO what happens when the agent reaches the GOAL ?\n",
    "        #TODO update reward and grid\n",
    "        if new_pos_char == \"G\":\n",
    "            reward = 1\n",
    "            \n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return tuple(self.agent_pos)\n",
    "\n",
    "    def get_state_space(self):\n",
    "        return list(np.ndindex(self.maze.shape))\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return list(self.action_movements.keys())\n",
    "\n",
    "    def get_num_states(self):\n",
    "        return len(self.get_state_space())\n",
    "\n",
    "    def get_num_actions(self):\n",
    "        return len(self.get_action_space())\n",
    "\n",
    "    def render(self):\n",
    "        'plot the maze'\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.imshow(self.maze == \"#\", vmin=0, vmax=1, cmap=\"Greys\")\n",
    "        h, w = self.maze.shape\n",
    "        ax.plot([-0.5, -0.5, w - 0.5, w - 0.5, -0.5],\n",
    "                [-0.5, h - 0.5, h - 0.5, -0.5, -0.5], 'k', lw=3)\n",
    "        y, x = self.agent_pos\n",
    "        self.agent_dot, = plt.plot((x,), (y,), 'ro', ms=20)\n",
    "        rew_y, rew_x = np.where(self.maze == \"G\")\n",
    "        ax.plot(rew_x, rew_y, \"b*\", ms=30)\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd35896",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridMaze' object has no attribute 'agent_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/saqib/iith/sem5/BM3000/SYED SAQIB HABEEB - RL Assignment.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m B \u001b[39m=\u001b[39m AbstractEnvironment()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m A \u001b[39m=\u001b[39m GridMaze()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(A\u001b[39m.\u001b[39;49mstep(\u001b[39m2\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m A\u001b[39m.\u001b[39mget_current_state()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m A\u001b[39m.\u001b[39mrender()\n",
      "\u001b[1;32m/home/saqib/iith/sem5/BM3000/SYED SAQIB HABEEB - RL Assignment.ipynb Cell 6\u001b[0m in \u001b[0;36mGridMaze.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m (action\u001b[39m!=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m action\u001b[39m!=\u001b[39m\u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m action\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m action\u001b[39m!=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid action \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m. Please use N, W, S or E.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m proxy_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent_pos \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_movements[action]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m ((proxy_pos)[\u001b[39m0\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m) \u001b[39mand\u001b[39;00m ((proxy_pos)[\u001b[39m0\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m ((proxy_pos)[\u001b[39m1\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m) \u001b[39mand\u001b[39;00m ((proxy_pos)[\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m   \u001b[39m#TODO determine the new position of the agent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saqib/iith/sem5/BM3000/SYED%20SAQIB%20HABEEB%20-%20RL%20Assignment.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m   new_pos \u001b[39m=\u001b[39m proxy_pos\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridMaze' object has no attribute 'agent_pos'"
     ]
    }
   ],
   "source": [
    "#testing gridmaze function\n",
    "B = AbstractEnvironment()\n",
    "A = GridMaze()\n",
    "print(A.step(2))\n",
    "A.get_current_state()\n",
    "A.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQLearner(AbstractAgent):\n",
    "    def __init__(self, env, alpha=0.02, beta=1, gamma=1, epsilon=0.5):\n",
    "        super().__init__(env)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        # Table of Q values with states on the rows and actions on the columns\n",
    "        self.Q_table = [[0.0 for j in range(self.env.get_num_actions())] for i in range(self.env.get_num_states())]\n",
    "        self.epsilon = epsilon  # exploration probability\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        #TODO implement the Q-Learning algorithm to train the agent\n",
    "        # get current state\n",
    "        state = self.env.get_current_state()\n",
    "        state_index = self.env.get_state_space().index(state)\n",
    "\n",
    "        # TODO select an action by epsilon greedy approach\n",
    "        action =\n",
    "        \n",
    "        # TODO perform the action and compute the reward\n",
    "        reward = self.env.step(action)\n",
    "        \n",
    "\n",
    "        # TODO update q table\n",
    "        self.Q_table[state_index][action] = \n",
    "        \n",
    "        return action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation\n",
    "import IPython.display\n",
    "\n",
    "\n",
    "class RLRunner:\n",
    "\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def step(self):\n",
    "        # TODO make the agent perform an action and record the reward\n",
    "\n",
    "    def run(self, n_steps):\n",
    "        #TODO train the agent for n_steps\n",
    "\n",
    "    def plot_cumulative_rewards(self):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        #TODO plot all the rewards obtained so far\n",
    "        ax.plot()\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(\"Rewards (cuml)\")\n",
    "\n",
    "    def plot_reward_rate(self, window_size=100):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        #TODO plot rewards obtained for every \"window_size\"\n",
    "        \n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(\"Reward/step\")\n",
    "\n",
    "    def plot_max_value_per_state(self):\n",
    "        #Plot max value per state as a heat map\n",
    "        \n",
    "        \n",
    "    def animate_history(self,fps=20, frames=None,):\n",
    "        if frames is None:\n",
    "            frames = len(self.states)\n",
    "        fig = self.agent.env.render()\n",
    "\n",
    "        def anim_update(i):\n",
    "            y, x = self.states[i]\n",
    "            self.agent.env.agent_dot.set_data((x,), (y,))\n",
    "\n",
    "        anim = matplotlib.animation.FuncAnimation(fig, anim_update,\n",
    "                                                  frames=frames,\n",
    "                                                  interval=1000.0 / fps)\n",
    "        \n",
    "        #TODO add your ffmpeg path from anaconda\n",
    "        plt.rcParams['animation.ffmpeg_path'] = \"/home/miniconda3/bin/ffmpeg\"\n",
    "\n",
    "        video = anim.to_html5_video()\n",
    "        plt.close(fig)\n",
    "        with open(\"myvideo.html\", \"w\") as f:\n",
    "            print(video, file=f)\n",
    "\n",
    "        #return IPython.display.HTML(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c142fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #TODO create a grid\n",
    "    #TODO create an agent\n",
    "    #TODO train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bba47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
